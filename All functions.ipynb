{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee667d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent\n",
  
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import time\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4229748",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0461d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLMtext(path, prompt,model, temperature):\n",
    "    df = pd.read_csv(path)\n",
    "    cgm_values =\",\".join(str(round(x)) for x in df['CGM'].values)\n",
    "    time_values =\",\".join(str(x) for x in df['Time'].values)\n",
    "    gpt_user_prompt = prompt +f\" CGM values: {cgm_values}, Time  values: {time_values}\"\n",
    "    client = OpenAI()\n",
    "    gpt_assistant_prompt = \"Estimate the answer from the text.\"\n",
    "\n",
    "    message=[{\"role\": \"system\", \"content\": gpt_assistant_prompt}, {\"role\": \"user\", \"content\": gpt_user_prompt}]\n",
    "\n",
    "    frequency_penalty=0.0\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages = message,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e06f9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLMcode(new_path, prompt,model, temperature,i=0,k=0):\n",
    "\n",
    "    ###############\n",
    "    ## Code adapted from\n",
    "    ### L. Cheng, X. Li and L. Bing, Is GPT-4 a good data analyst?, arXiv [cs.CL] (May 2023).\n",
    "    ###############\n",
    "    df = pd.read_csv(new_path)\n",
    "    small = df[['Time','CGM']]\n",
    "    new_path = 'temp_files/_temp.txt'\n",
    "    small.to_csv(new_path, sep='\\t', index=False)\n",
    "    small.to_csv(new_path)\n",
    "    def get_gpt_result(system_role, question, max_tokens, temperature):\n",
    "        client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_role},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        )\n",
    "        return response \n",
    "    \n",
    "    max_tokens = 2000\n",
    "    system_role = '''Write code in Python to answer the question where the glucose column is \"CGM\" and the time column is \"Time\". All necessary packages are already installed.  \n",
    "    Please save the answer in a file \"answer.txt\". '''\n",
    "    question =\"Question: \" + prompt +' df = pd.read_csv('+new_path+')'\n",
    " \n",
    "    response = get_gpt_result(system_role, question, max_tokens, float(temperature))\n",
    "    text = response.choices[0].message.content\n",
    "    print(text)\n",
    "    try:\n",
    "        string =text\n",
    "        substring = \"```\"\n",
    "        indexes = []\n",
    "        index = string.find(substring)\n",
    "        while index != -1:\n",
    "            indexes.append(index)\n",
    "            index = string.find(substring, index + 1)\n",
    "        python = text[indexes[0] + 10:indexes[1]]\n",
    "    except:\n",
    "        python = text\n",
    "\n",
    "    file = open(f\"analyzecgm.py\", \"w\")\n",
    "    file.write(python)\n",
    "    file.close()\n",
    "\n",
    "    os.system(f\"python analyzecgm.py\")\n",
    "    data = open(f\"answer.txt\", 'r').read()\n",
    "    \n",
    "    question = \"Question: \" + prompt + '\\nData: \\n' + data\n",
    "    system_role = 'Generate response.'\n",
    "\n",
    "    response = get_gpt_result(system_role, question, max_tokens, temperature)\n",
    "    text = response.choices[0].message.content\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b93b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLMcodechain(path, prompt,model, temperature):\n",
    "    df = pd.read_csv(path)\n",
    "    small = df[['Time','CGM']]\n",
    "    new_path = 'temp_files/_temp.txt'\n",
    "    small.to_csv(new_path, sep='\\t', index=False)\n",
    "    small.to_csv(new_path)\n",
    "    agent = create_csv_agent(\n",
    "        ChatOpenAI(temperature=temperature, model=model),\n",
    "        new_path,\n",
    "        verbose=True,return_intermediate_steps=True\n",
    "        )\n",
    "    response_= agent(prompt)\n",
    "    response = response_['output']\n",
    "    steps = response_['intermediate_steps']\n",
    "    return response, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72ac69",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc8d95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions = ['Q1:What was my mean glucose?',\n",
    "'Q2:What was my maximum glucose?',\n",
    "'Q3:What was the standard deviation of my glucose? ',\n",
    "'Q4:What was my minimum glucose?',\n",
    "'Q5:What was my percent time in range? ',\n",
    "'Q6:What was my percent time in hyperglycemia?',\n",
    "'Q7:What was my percent time in hypoglycemia?',\n",
    "'Q8:What was my glycemic variability?',\n",
    "'Q9:What was my percent time in severe hyperglycemia?',\n",
    "'Q10:What is my estimated A1C?',\n",
    "'Q11:What was my percent time in severe hypoglycemia?',\n",
    "             \n",
    "'Q12:What time was my blood glucose highest?',\n",
    "'Q13:What day was my glucose control the most out of range?',\n",
    "'Q14:What time of the day was my blood glucose lowest? ',\n",
    "'Q15:When did my most recent episode of hypoglycemia start?',\n",
    "'Q16:How long was my last episode of hypoglycemia?',\n",
    "'Q17:What was my longest time spent in hyperglycemia? ',\n",
    "'Q18:How many times did I experience hypoglycemia? ',\n",
    "'Q19:What was my mean overnight blood glucose? ',\n",
    "'Q20:What period of the day did I have the highest blood glucose?',\n",
    "'Q21:Did I have noctural hypoglycemia? ',\n",
    "'Q22:What was my highest glucose reading during dinner? ',\n",
    "\n",
    "'Q23:What percent of time was my CGM active?',\n",
    "'Q24:How many times did my sensor disconnect?',\n",
    "'Q25:Was my low blood glucose likely due to sensor error?',\n",
    "'Q26:Are there any artifacts in the CGM data?',\n",
    "                  \n",
    "'Q27:Was my average glucose control today better than yesterday?',\n",
    "'Q28:Was my time in range improved this week compared to last week? ',\n",
    "'Q29:Was my max glucose lower today than yesterday?',\n",
    "'Q30:Did I spend less time in hypoglycemia this week than last week?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a078fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-0125-preview\"\n",
    "temperature= .1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ffeb6",
   "metadata": {},
   "source": [
    "## Iterate through Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b424ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "k=0\n",
    "for prompt in Questions:\n",
    "    prompt = prompt[4:]\n",
    "    print('\\n\\n ---')\n",
    "    time.sleep(2)\n",
    "    print(prompt)\n",
    "    print('\\n\\n')\n",
    "    k=k+1\n",
    "    all_results[f'Q{k}'] = {}\n",
    "    all_results['Question']=prompt\n",
    "\n",
    "\n",
    "    new_path = f'data.csv'\n",
    "    all_results[f'Q{k}']={}\n",
    "    savepath=f'files/dat_{k}'\n",
    "    if not os.path.exists(savepath):\n",
    "        os.makedirs(savepath)\n",
    "\n",
    "    try:\n",
    "        print('\\nCode:')\n",
    "        response = LLMcode(new_path, prompt,model, temperature,i,k)\n",
    "        print(response)\n",
    "        all_results[f'Q{k}']['LLM-code'] = response\n",
    "        shutil.move('answer.txt',savepath+'/out.txt') \n",
    "        shutil.move('analyzecgm.py',savepath+'/run.py') \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f'{k}  failed')      \n",
    "    try:\n",
    "        print('\\nCodeChain:')\n",
    "        response,steps =LLMcodechain(new_path, prompt,model, temperature)\n",
    "        print(response)\n",
    "        all_results[f'Q{k}']['LLM-codechain'] = response\n",
    "\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f'{k} failed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ODE_env]",
   "language": "python",
   "name": "conda-env-ODE_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
